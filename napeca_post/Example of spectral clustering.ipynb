{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1) PCA to reduce dimensionality of trial-averaged event-related responses (rois x time), with respect to time dimension. Intuitive concept: each ROI's datapoint resides in n dimensional space where n is the number of samples in the event-related window. PCA finds new set of (orthogonal) axes that maximizes the variance in the activity. These new axes are linear combinations of the original axes\n",
    "\n",
    "2) Spectral clustering: The roi data are now characterized by a reduced set of optimized axes describing time. We now cluster using spectral clustering, which does not assume any particular shape of the cluster data points (eg. kmeans assumes data clouds are gaussian). The three main steps of spectral clustering are **A)** create graph theory similarity matrix for each ROI based on how close other ROIs are in the PCA space, **B)** perform eigendecomposition of the similarity matrix, **C)** Use kmeans clustering on the transformed data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import math\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC, SVR, LinearSVC\n",
    "from sklearn.metrics import accuracy_score, silhouette_score, adjusted_rand_score, silhouette_samples\n",
    "from sklearn.cluster import AgglomerativeClustering, SpectralClustering, KMeans\n",
    "from sklearn.model_selection import KFold, LeaveOneOut, train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn import linear_model\n",
    "from sklearn.manifold import TSNE\n",
    "import scipy.stats as stats\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from patsy import (ModelDesc, EvalEnvironment, Term, EvalFactor, LookupFactor, dmatrices, INTERCEPT)\n",
    "from statsmodels.distributions.empirical_distribution import ECDF\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.colorbar as colorbar\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_plot_graphics(ax):\n",
    "    [i.set_linewidth(0.5) for i in ax.spines.itervalues()]\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    return ax\n",
    "\n",
    "def fit_regression(x, y):\n",
    "    lm = sm.OLS(y, sm.add_constant(x)).fit()\n",
    "    x_range = sm.add_constant(np.array([x.min(), x.max()]))\n",
    "    x_range_pred = lm.predict(x_range)\n",
    "    return lm.pvalues[1], lm.params[1], x_range[:,1], x_range_pred, lm.rsquared\n",
    "\n",
    "def CDFplot(x, ax, **kwargs):\n",
    "    x = np.array(x)\n",
    "    ix=np.argsort(x)\n",
    "    ax.plot(x[ix], ECDF(x)(x)[ix], **kwargs)\n",
    "    return ax\n",
    "\n",
    "\n",
    "def fit_regression_and_plot(x, y, ax, plot_label='', color='k', linecolor='r', markersize=3,\n",
    "                            show_pval=True):\n",
    "    #linetype is a string like 'bo'\n",
    "    pvalue, slope, temp, temppred, R2 = fit_regression(x, y)   \n",
    "    if show_pval:\n",
    "        plot_label = '%s p=%.2e\\nr=%.3f'% (plot_label, pvalue, np.sign(slope)*np.sqrt(R2))\n",
    "    else:\n",
    "        plot_label = '%s r=%.3f'% (plot_label, np.sign(slope)*np.sqrt(R2))\n",
    "    ax.scatter(x, y, color=color, label=plot_label, s=markersize)\n",
    "    ax.plot(temp, temppred, color=linecolor)\n",
    "    return ax, slope, pvalue, R2\n",
    "\n",
    "\n",
    "def make_silhouette_plot(X, cluster_labels):\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    n_clusters = len(set(cluster_labels))\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "    fig.set_size_inches(4, 4)\n",
    "\n",
    "    # The 1st subplot is the silhouette plot\n",
    "    # The silhouette coefficient can range from -1, 1 but in this example all\n",
    "    # lie within [-0.1, 1]\n",
    "    ax.set_xlim([-0.4, 1])\n",
    "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "    # plots of individual clusters, to demarcate them clearly.\n",
    "    ax.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n",
    "    silhouette_avg = silhouette_score(X, cluster_labels, metric='cosine')\n",
    "\n",
    "    # Compute the silhouette scores for each sample\n",
    "    sample_silhouette_values = silhouette_samples(X, cluster_labels, metric='cosine')\n",
    "\n",
    "    y_lower = 10\n",
    "    for i in range(n_clusters):\n",
    "        # Aggregate the silhouette scores for samples belonging to\n",
    "        # cluster i, and sort them\n",
    "        ith_cluster_silhouette_values = \\\n",
    "            sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = colors_for_cluster[i]\n",
    "        ax.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                          0, ith_cluster_silhouette_values,\n",
    "                          facecolor=color, edgecolor=color, alpha=0.9)\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        ax.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i+1))\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    ax.set_title(\"The silhouette plot for the various clusters.\")\n",
    "    ax.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    ax.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax.set_xticks([-0.4, -0.2, 0, 0.2, 0.4, 0.6, 0.8, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Ken Analysis\"\"\"\n",
    "\n",
    "# Load the data\n",
    "# basedir = r'\\\\172.25.144.34\\LabCommon\\Ken\\data\\2pImaging\\result\\result_pickle' # folder containing code and data\n",
    "# populationdata = np.load(os.path.join(basedir, 'normal_cycle_female_23062020_140612_Alignedtotrial_meansignal_norm_sucrose_low_high estrogen.npy'),allow_pickle = True)\n",
    "# print(str(populationdata.shape[0])+\" cells were analyzed\")\n",
    "\n",
    "\n",
    "# dt_string = 'normal_cycle_female_23062020_140612_'\n",
    "# normalization = 'mean'\n",
    "# signal_type = \"signal_norm\" #'signal_norm'  \n",
    "# analysis_condition = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = '43384-1 400pwr 1104um 16x obj water-004'\n",
    "fdir = r'D:\\bruker_data\\Sean\\43384-1 400pwr 1104um 16x obj water-004'\n",
    "\n",
    "save_dir = os.path.join(fdir, 'event_rel_analysis', 'clustering')\n",
    "\n",
    "# import data and define trial types\n",
    "data_dict_path = os.path.join(fdir, 'event_rel_analysis\\event_data_dict.pkl')\n",
    "with open(data_dict_path, 'rb') as pkl_handle:\n",
    "    data_dict = pickle.load(pkl_handle)\n",
    "trial_types =  data_dict.keys() #['low estrogen', 'high estrogen'] \n",
    "\n",
    "with open(os.path.join(fdir, 'event_analysis_fparam')) as json_file: \n",
    "    event_analysis_fparam = json.load(json_file) \n",
    "framerate = event_analysis_fparam['fs']\n",
    "second_event_seconds = 1\n",
    "\n",
    "# concatenates data across trials in the time axis; populationdata dimentionss are ROI by time (trials are appended)\n",
    "populationdata = np.concatenate([data_dict[condition]['ztrial_avg_data'] for condition in trial_types], axis=1)\n",
    "num_samples_trial = populationdata.shape[-1]/len(trial_types)\n",
    "\n",
    "nan_rows = np.unique(np.where(np.isnan(populationdata))[0])\n",
    "if nan_rows.size != 0:\n",
    "    populationdata = np.delete(populationdata, obj=nan_rows, axis=0)\n",
    "    print('Some ROIs contain nan in tseries!')\n",
    "num_rois = populationdata.shape[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declaration of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables for plotting\n",
    "pre_window_size = event_analysis_fparam['trial_start_end'][0]*framerate # n seconds multipled by framerate. Corresponds to baseline period prior to cues/event.\n",
    "flag_plot_reward_line = False\n",
    "frames_to_reward = second_event_seconds*framerate \n",
    "\n",
    "# define time vector for converting samples in a trial to seconds\n",
    "tvec = np.linspace(event_analysis_fparam['trial_start_end'][0], event_analysis_fparam['trial_start_end'][1], num_samples_trial)\n",
    "\n",
    "#variables for clustering\n",
    "max_n_clusters = 10# Maximum number of clusters expected. I already ran this with up to 20 clusters and know\n",
    "# that the optimal number is 9. So, I am leaving this at 11. In your data, might be worth increasing this, but\n",
    "# it will take more time to run.\n",
    "\n",
    "'''In spectral clustering: get n nearest neighbors for each data point and create connectivity graph (affinity matrix)'''\n",
    "possible_n_nearest_neighbors = np.array([3,5,10]) # This should be selected for each dataset\n",
    "# appropriately. When 4813 neurons are present, the above number of nearest neighbors ([30,40,30,50,60]) provides a good sweep of the\n",
    "# parameter space. But it will need to be changed for other data.\n",
    "\n",
    "# calculated variables\n",
    "window_size = populationdata.shape[1]/2 # Total number of frames plotted around a cue\n",
    "sortwindow_frames = [int(np.round(time*framerate)) for time in event_analysis_fparam['event_sort_win']] # Sort responses between first lick and 10 seconds.\n",
    "sortresponse = np.argsort(np.mean(populationdata[:,sortwindow_frames[0]:sortwindow_frames[1]], axis=1))[::-1]\n",
    "# sortresponse corresponds to an ordering of the neurons based on their average response in the sortwindow\n",
    "\n",
    "cmax = np.nanmax(np.abs([np.nanmin(populationdata), np.nanmax(populationdata)])) # Maximum colormap value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2,2,figsize=(3*2,3*2), sharex='all', sharey='row')\n",
    "\n",
    "# loop through conditions and plot heatmaps of trial-avged activity\n",
    "for t in range(len(trial_types)):\n",
    "    \n",
    "    ax = axs[0,t]\n",
    "\n",
    "    plot_extent = [tvec[0], tvec[-1], num_rois, 0 ]\n",
    "    im = utils.subplot_heatmap(ax, ' ', populationdata[sortresponse, t*window_size: (t+1)*window_size], \n",
    "                               clims = [-cmax, cmax], extent_=plot_extent)\n",
    "    axs[0,t].set_title(trial_types[t])\n",
    "    \n",
    "    ax.axvline(0, linestyle='--', color='k', linewidth=0.5)   \n",
    "    if flag_plot_reward_line:\n",
    "        ax.axvline(0, linestyle='--', color='k', linewidth=0.5)    \n",
    "    \n",
    "    ### roi-avg tseries \n",
    "    ax = axs[1,t]\n",
    "    sns.tsplot(populationdata[sortresponse, t*window_size:(t+1)*window_size],\n",
    "               ax=ax, time=tvec)\n",
    "    ax.axvline(0, linestyle='--', color='k', linewidth=0.5)  \n",
    "    if flag_plot_reward_line:\n",
    "        ax.axvline(second_event_seconds, linestyle='--', color='k', linewidth=0.5)   \n",
    "    ax.set_xlabel('Time from event (s)')   \n",
    " \n",
    "axs[0,0].set_ylabel('Neurons')\n",
    "axs[1,0].set_ylabel('Mean norm. fluor.')\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "cbar = fig.colorbar(im, ax = axs, shrink = 0.7)\n",
    "cbar.ax.set_ylabel('Heatmap Z-Score Activity', fontsize=13);\n",
    "\n",
    "# fig.savefig(os.path.join(save_dir, 'results', tempstr+'.pdf'), format='pdf')\n",
    "#fig.savefig(os.path.join(save_dir, dt_string+'_'+clusterkey + '+' + trial_types[0] + '_'+trial_types[1]+'.png'), format='png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do PCA to reduce dimensionality in the time-domain\n",
    "\n",
    "PCA: A linear algebra-based method to optimize how a set of variables can explain the variability of a dataset. Optimizing: meaning finding a new set of axes (ie. variables) that are linear combinations of the original axes where each new axis attempts to capture the most amount of variability in the data as possible while remaining linearly independent from the other new axes.\n",
    "\n",
    "In this case, we are finding a new linearly independent parameter space that maximizes the explained variance into the top new axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_pc_explained_var(explained_var, explained_var_thresh=90):\n",
    "    cum_sum = 0\n",
    "    for idx, PC_var in enumerate(explained_var):\n",
    "        cum_sum += PC_var\n",
    "        if cum_sum > explained_var_thresh:\n",
    "            return idx+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "load_savedpca_or_dopca = 'dopca'\n",
    "# Select 'dopca' for doing PCA on the data. Select 'savedpca' for loading my previous results\n",
    "\n",
    "# perform PCA across time\n",
    "if load_savedpca_or_dopca == 'dopca':\n",
    "    pca = PCA(n_components=min(populationdata.shape[0],populationdata.shape[1]), whiten=True)\n",
    "    pca.fit(populationdata) \n",
    "    with open(os.path.join(basedir, 'pcaresults.pickle'), 'wb') as f:\n",
    "        pickle.dump(pca, f)\n",
    "elif load_savedpca_or_dopca == 'savedpca':\n",
    "    with open(os.path.join(basedir, 'OFCCaMKII_pcaresults.pickle'), 'rb') as f:\n",
    "        pca = pickle.load(f)\n",
    "\n",
    "# pca across time\n",
    "transformed_data = pca.transform(populationdata)\n",
    "# transformed data: each ROI is now a linear combination of the original time-serie\n",
    "#np.save(os.path.join(save_dir, dt_string+'_'+clusterkey+'_' + \"transformed_data.npy\"),transformed_data)\n",
    "\n",
    "# grab eigenvectors; linear combination of original axes\n",
    "pca_vectors = pca.components_\n",
    "print 'Number of PCs = %d'%(pca_vectors.shape[0])\n",
    "\n",
    "# Number of PCs to be kept is defined as the number at which the \n",
    "# scree plot bends. This is done by simply bending the scree plot\n",
    "# around the line joining (1, variance explained by first PC) and\n",
    "# (num of PCs, variance explained by the last PC) and finding the \n",
    "# number of components just below the minimum of this rotated plot\n",
    "x = 100*pca.explained_variance_ratio_ # eigenvalue ratios\n",
    "xprime = x - (x[0] + (x[-1]-x[0])/(x.size-1)*np.arange(x.size))\n",
    "\n",
    "# define number of PCs\n",
    "num_retained_pcs = np.argmin(xprime) #num_pc_explained_var(x, 90) #\n",
    "print 'Number of PCs to keep = %d'%(num_retained_pcs)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(2,2))\n",
    "ax.plot(np.arange(pca.explained_variance_ratio_.shape[0]).astype(int)+1, x, 'k')\n",
    "ax.set_ylabel('Percentage of\\nvariance explained')\n",
    "ax.set_xlabel('PC number')\n",
    "ax.axvline(num_retained_pcs, linestyle='--', color='k', linewidth=0.5)\n",
    "ax.set_title('Scree plot')\n",
    "# ax.set_xlim([0,50])\n",
    "[i.set_linewidth(0.5) for i in ax.spines.itervalues()]\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "\n",
    "fig.subplots_adjust(left=0.3)\n",
    "fig.subplots_adjust(right=0.98)\n",
    "fig.subplots_adjust(bottom=0.25)\n",
    "fig.subplots_adjust(top=0.9)\n",
    "#fig.savefig(os.path.join(save_dir, dt_string+'_'+clusterkey+'_' + trial_types[0] + '_'+trial_types[1]+'_scree_plot.png'), format='png', dpi=300)\n",
    "\n",
    "\n",
    "colors_for_key = {}\n",
    "colors_for_key[trial_types[0]] = (0,0.5,1)\n",
    "colors_for_key[trial_types[1]] = (1,0.5,0)\n",
    "\n",
    "### plot retained principal components\n",
    "numcols = 2.0\n",
    "fig, axs = plt.subplots(int(np.ceil(num_retained_pcs/numcols)), int(numcols), sharey='all',\n",
    "                        figsize=(2.2*numcols, 2.2*int(np.ceil(num_retained_pcs/numcols))))\n",
    "for pc in range(num_retained_pcs):\n",
    "    ax = axs.flat[pc]\n",
    "    for k, tempkey in enumerate(trial_types):\n",
    "        ax.plot(tvec, pca_vectors[pc, k*window_size:(k+1)*window_size], color=colors_for_key[tempkey],\n",
    "                label='PC %d: %s'%(pc+1, tempkey))\n",
    "        \n",
    "    ax.axvline(0, linestyle='--', color='k', linewidth=1)\n",
    "    ax.set_title('PC %d'%(pc+1))\n",
    "    \n",
    "    # labels\n",
    "    if pc == 0:\n",
    "        ax.set_xlabel('Time from cue (s)')\n",
    "        ax.set_ylabel( 'PCA weights')\n",
    "\n",
    "\n",
    "fig.tight_layout()\n",
    "for ax in axs.flat[num_retained_pcs:]:\n",
    "    ax.set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "#fig.savefig(os.path.join(save_dir, dt_string+'_'+clusterkey+'_' +  trial_types[0] + '_'+trial_types[1]+'_PCA.png'), format='png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spectral Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# calculate optimal number of clusters and nearest neighbors using silhouette scores\n",
    "min_clusters = np.min([max_n_clusters+1, int(populationdata.shape[0])])\n",
    "possible_n_clusters = np.arange(2, max_n_clusters+1) #This requires a minimum of 2 clusters.\n",
    "# When the data contain no clusters at all, it will be quite visible when inspecting the two obtained clusters, \n",
    "# as the responses of the clusters will be quite similar. This will also be visible when plotting the data in\n",
    "# the reduced dimensionality PC space (done below).\n",
    "\n",
    "    \n",
    "silhouette_scores = np.nan*np.ones((possible_n_clusters.size,\n",
    "                                    possible_n_nearest_neighbors.size))\n",
    "\n",
    "# loop through iterations of spectral clustering params\n",
    "for n_clustersidx, n_clusters in enumerate(possible_n_clusters):\n",
    "    for nnidx, nn in enumerate(possible_n_nearest_neighbors):\n",
    "        model = SpectralClustering(n_clusters=n_clusters, affinity='nearest_neighbors', n_neighbors=nn)\n",
    "        model.fit(transformed_data[:,:num_retained_pcs])\n",
    "        silhouette_scores[n_clustersidx, nnidx] = silhouette_score(transformed_data[:,:num_retained_pcs],\n",
    "                                                                   model.labels_,\n",
    "                                                                   metric='cosine')\n",
    "        print 'Done with numclusters = %d, num nearest neighbors = %d: score = %.3f'%(n_clusters,\n",
    "                                                                                      nn,\n",
    "                                                                                      silhouette_scores[n_clustersidx,                                                                           \n",
    "                                                                                                        nnidx])\n",
    "\n",
    "print 'Done with model fitting'\n",
    "\n",
    "silhouette_dict = {}\n",
    "silhouette_dict['possible_n_clusters'] = possible_n_clusters\n",
    "silhouette_dict['possible_n_nearest_neighbors'] = possible_n_nearest_neighbors\n",
    "silhouette_dict['silhouette_scores'] = silhouette_scores\n",
    "silhouette_dict['shape'] = 'cluster_nn'\n",
    "#with open(os.path.join(save_dir,dt_string+'_'+ clusterkey+'_' +  'silhouette_scores.pickle'), 'wb') as f:\n",
    "#    pickle.dump(temp, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recluster with optimal params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify optimal parameters from the above parameter space\n",
    "temp = np.where(silhouette_dict['silhouette_scores']==np.nanmax(silhouette_dict['silhouette_scores']))\n",
    "n_clusters = silhouette_dict['possible_n_clusters'][temp[0][0]]\n",
    "n_nearest_neighbors = silhouette_dict['possible_n_nearest_neighbors'][temp[1][0]]\n",
    "\n",
    "print n_clusters, n_nearest_neighbors\n",
    "\n",
    "# Redo clustering with these optimal parameters\n",
    "model = SpectralClustering(n_clusters=n_clusters,\n",
    "                           affinity='nearest_neighbors',\n",
    "                           n_neighbors=n_nearest_neighbors)\n",
    "\n",
    "# model = KMeans(n_clusters=n_clusters)\n",
    "\n",
    "# model = AgglomerativeClustering(n_clusters=9,\n",
    "#                                 affinity='l1',\n",
    "#                                 linkage='average')\n",
    "\n",
    "model.fit(transformed_data[:,:num_retained_pcs])\n",
    "\n",
    "temp = silhouette_score(transformed_data[:,:num_retained_pcs], model.labels_, metric='cosine')\n",
    "\n",
    "print 'Number of clusters = %d, average silhouette = %.3f'%(len(set(model.labels_)), temp)\n",
    "\n",
    "# Save this optimal clustering model.\n",
    "# with open(os.path.join(save_dir, 'clusteringmodel.pickle'), 'wb') as f:\n",
    "#     pickle.dump(model, f)\n",
    "\n",
    "          \n",
    "# Since the clustering labels are arbitrary, I rename the clusters so that the first cluster will have the most\n",
    "# positive response and the last cluster will have the most negative response.\n",
    "def reorder_clusters(data, sort_win_frames, rawlabels):\n",
    "    uniquelabels = list(set(rawlabels))\n",
    "    responses = np.nan*np.ones((len(uniquelabels),))\n",
    "    for l, label in enumerate(uniquelabels):\n",
    "        responses[l] = np.mean(data[rawlabels==label, sort_win_frames[0]:sort_win_frames[1]])\n",
    "    temp = np.argsort(responses).astype(int)[::-1]\n",
    "    temp = np.array([np.where(temp==a)[0][0] for a in uniquelabels])\n",
    "    outputlabels = np.array([temp[a] for a in list(np.digitize(rawlabels, uniquelabels)-1)])\n",
    "    return outputlabels\n",
    "newlabels = reorder_clusters(populationdata, sortwindow_frames, model.labels_)\n",
    "\n",
    "# Create a new variable containing all unique cluster labels\n",
    "uniquelabels = list(set(newlabels))\n",
    "\n",
    "# np.save(os.path.join(summarydictdir, dt_string+'_'+ clusterkey+'_' + 'spectral_clusterlabels.npy'), newlabels)\n",
    "\n",
    "colors_for_cluster = [[0.933, 0.250, 0.211],\n",
    "                      [0.941, 0.352, 0.156],\n",
    "                      [0.964, 0.572, 0.117],\n",
    "                      [0.980, 0.686, 0.250],\n",
    "                      [0.545, 0.772, 0.247],\n",
    "                      [0.215, 0.701, 0.290],\n",
    "                      [0, 0.576, 0.270],\n",
    "                      [0, 0.650, 0.611],\n",
    "                      [0.145, 0.662, 0.878]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "sortwindow = [15, 100]\n",
    "\n",
    "fig, axs = plt.subplots(len(trial_types),len(uniquelabels),\n",
    "                        figsize=(2*len(uniquelabels),2*len(trial_types)))\n",
    "if len(axs.shape) == 1:\n",
    "    axs = np.expand_dims(axs, axis=0)\n",
    "\n",
    "numroisincluster = np.nan*np.ones((len(uniquelabels),))\n",
    "\n",
    "for c, cluster in enumerate(uniquelabels):\n",
    "    for k, tempkey in enumerate(trial_types):\n",
    "        temp = populationdata[np.where(newlabels==cluster)[0], k*window_size:(k+1)*window_size]\n",
    "        numroisincluster[c] = temp.shape[0]\n",
    "        ax=axs[k, cluster]\n",
    "        sortresponse = np.argsort(np.mean(temp[:,sortwindow[0]:sortwindow[1]], axis=1))[::-1]\n",
    "        \n",
    "        plot_extent = [tvec[0], tvec[-1], len(sortresponse), 0 ]\n",
    "        im = utils.subplot_heatmap(ax, ' ', temp[sortresponse], \n",
    "                                   clims = [-cmax, cmax], extent_=plot_extent)\n",
    "\n",
    "        axs[k, cluster].grid(False) \n",
    "        if k!=len(trial_types)-1:\n",
    "\n",
    "            axs[k, cluster].set_xticks([])\n",
    "\n",
    "        axs[k, cluster].set_yticks([])\n",
    "        axs[k, cluster].axvline(0, linestyle='--', color='k', linewidth=0.5)\n",
    "        if flag_plot_reward_line:\n",
    "            axs[k, cluster].axvline(second_event_seconds, linestyle='--', color='k', linewidth=0.5)\n",
    "        if cluster==0:\n",
    "            axs[k, 0].set_ylabel('%s'%(tempkey))\n",
    "    axs[0, cluster].set_title('Cluster %d\\n(n=%d)'%(cluster+1, numroisincluster[c]))\n",
    "    \n",
    "fig.text(0.5, 0.05, 'Time from cue (s)', fontsize=12,\n",
    "         horizontalalignment='center', verticalalignment='center', rotation='horizontal')\n",
    "fig.tight_layout()\n",
    "\n",
    "fig.subplots_adjust(wspace=0.1, hspace=0.1)\n",
    "fig.subplots_adjust(left=0.03)\n",
    "fig.subplots_adjust(right=0.93)\n",
    "fig.subplots_adjust(bottom=0.2)\n",
    "fig.subplots_adjust(top=0.83)                                    \n",
    "\n",
    "cbar = fig.colorbar(im, ax = axs, shrink = 0.7)\n",
    "cbar.ax.set_ylabel('Z-Score Activity', fontsize=13);\n",
    "\n",
    "#fig.savefig(os.path.join(save_dir, dt_string+'_'+clusterkey+'_' +  trial_types[0] + '_'+trial_types[1]+'_clusters_spectral_clustering.png'),format='png', dpi=300,bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvec_convert_dict = {}\n",
    "for i in range(len(tvec)):\n",
    "    tvec_convert_dict[i] = tvec[i] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, axs = plt.subplots(1,len(uniquelabels),\n",
    "                        figsize=(3*len(uniquelabels),1.3*len(trial_types)))\n",
    "\n",
    "for c, cluster in enumerate(uniquelabels):\n",
    "\n",
    "    for k, tempkey in enumerate(trial_types):\n",
    "        temp = populationdata[np.where(newlabels==cluster)[0], k*window_size:(k+1)*window_size]\n",
    "        numroisincluster[c] = temp.shape[0]\n",
    "        sortresponse = np.argsort(np.mean(temp[:,sortwindow[0]:sortwindow[1]], axis=1))[::-1]\n",
    "        sns.lineplot(x=\"variable\", y=\"value\",data = pd.DataFrame(temp[sortresponse]).rename(columns=tvec_convert_dict).melt(),\n",
    "                    ax = axs[cluster],\n",
    "                    palette=plt.get_cmap('coolwarm'),label = tempkey,legend = False)\n",
    "        axs[cluster].grid(False)  \n",
    "        axs[cluster].axvline(0, linestyle='--', color='k', linewidth=0.5)\n",
    "#         axs[cluster].axvline(pre_window_size + frames_to_reward, linestyle='--', color='k', linewidth=0.5)\n",
    "        axs[cluster].spines['right'].set_visible(False)\n",
    "        axs[cluster].spines['top'].set_visible(False)\n",
    "        if cluster==0:\n",
    "            axs[cluster].set_ylabel('Normalized fluorescence')\n",
    "        else:\n",
    "            axs[cluster].set_ylabel('')\n",
    "        axs[cluster].set_xlabel('')\n",
    "    axs[cluster].set_title('Cluster %d\\n(n=%d)'%(cluster+1, numroisincluster[c]))\n",
    "    axs[0].legend()\n",
    "fig.text(0.5, 0.05, 'Time from cue (s)', fontsize=12,\n",
    "         horizontalalignment='center', verticalalignment='center', rotation='horizontal')\n",
    "fig.tight_layout()\n",
    "\n",
    "fig.subplots_adjust(wspace=0.1, hspace=0.1)\n",
    "fig.subplots_adjust(left=0.03)\n",
    "fig.subplots_adjust(right=0.93)\n",
    "fig.subplots_adjust(bottom=0.2)\n",
    "fig.subplots_adjust(top=0.83)\n",
    "\n",
    "#fig.savefig(os.path.join(save_dir, dt_string+'_'+ clusterkey+'_' + trial_types[0] + '_'+trial_types[1]+'_clusters_average_spectral_clustering.png'), format='png', dpi=300,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusterpairs = len(uniquelabels)*(len(uniquelabels)-1)/2\n",
    "\n",
    "numrows = int(np.ceil(num_clusterpairs**0.5))\n",
    "numcols = int(np.ceil(num_clusterpairs/np.ceil(num_clusterpairs**0.5)))\n",
    "fig, axs = plt.subplots(numrows, numcols, figsize=(3*numrows, 3*numcols))\n",
    "\n",
    "tempsum = 0\n",
    "for c1, cluster1 in enumerate(uniquelabels):\n",
    "    for c2, cluster2 in enumerate(uniquelabels):\n",
    "        if cluster1>=cluster2:\n",
    "            continue\n",
    "        temp1 = transformed_data[np.where(newlabels==cluster1)[0], :num_retained_pcs]\n",
    "        temp2 = transformed_data[np.where(newlabels==cluster2)[0], :num_retained_pcs]\n",
    "        X = np.concatenate((temp1, temp2), axis=0)\n",
    "        tsne = TSNE(n_components=2, init='random',\n",
    "                    random_state=0, perplexity=100)\n",
    "        Y = tsne.fit_transform(X)\n",
    "        if numrows*numcols==1:\n",
    "            ax = axs\n",
    "        else:\n",
    "            ax = axs[tempsum/numcols,\n",
    "                     tempsum - tempsum/numcols*numcols]   \n",
    "        ax.scatter(Y[:np.sum(newlabels==cluster1),0],\n",
    "                   Y[:np.sum(newlabels==cluster1),1],\n",
    "                   color=colors_for_cluster[cluster1], label='Cluster %d'%(cluster1+1), alpha=1)\n",
    "        ax.scatter(Y[np.sum(newlabels==cluster1):,0],\n",
    "                   Y[np.sum(newlabels==cluster1):,1],\n",
    "                   color=colors_for_cluster[cluster2+3], label='Cluster %d'%(cluster2+1), alpha=1)\n",
    "\n",
    "        ax.set_xlabel('tsne dimension 1')\n",
    "        ax.set_ylabel('tsne dimension 2')\n",
    "        ax.legend()\n",
    "        tempsum += 1\n",
    "\n",
    "        fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
